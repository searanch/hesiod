{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project analyzing works of Ancient Greek Poet Hesiod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Text from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File\n",
    "# DO NOT RUN\n",
    "f = open('hesoid.txt','r', encoding=\"utf8\")\n",
    "raw = f.read()\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100215"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Tokenize \n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# Checking lenght\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension\n",
    "tokens_without_sw = [word for word in tokens if not word in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62022"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# checking length to confirm stopwords were removed\n",
    "len(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize sentences\n",
    "sentences = sent_tokenize(raw)\n",
    "#sentences"
   ]
  },
  {
   "source": [
    "PUT sentences in DF\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    tokenized_sentences\n",
       "765   There will be no favour for the man who keeps ...\n",
       "578                                 Γ   Brussels, Bibl.\n",
       "898   (ll 493-501) Pass by the smithy and its crowde...\n",
       "3248                          17-19) Hail to you, lord!\n",
       "1343  The\\nhot vapour lapped round the earthborn Tit...\n",
       "583         G    Rome, Vatican, Regina 91 (16th cent.).\n",
       "3426  Here, too, Achilles quarrels with Agamemnon\\nb...\n",
       "1589  So she bare sons to the almighty Son of Cronos...\n",
       "2180  And he\\nsays there were four of them, Argus, P..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokenized_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>765</th>\n      <td>There will be no favour for the man who keeps ...</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>Γ   Brussels, Bibl.</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>(ll 493-501) Pass by the smithy and its crowde...</td>\n    </tr>\n    <tr>\n      <th>3248</th>\n      <td>17-19) Hail to you, lord!</td>\n    </tr>\n    <tr>\n      <th>1343</th>\n      <td>The\\nhot vapour lapped round the earthborn Tit...</td>\n    </tr>\n    <tr>\n      <th>583</th>\n      <td>G    Rome, Vatican, Regina 91 (16th cent.).</td>\n    </tr>\n    <tr>\n      <th>3426</th>\n      <td>Here, too, Achilles quarrels with Agamemnon\\nb...</td>\n    </tr>\n    <tr>\n      <th>1589</th>\n      <td>So she bare sons to the almighty Son of Cronos...</td>\n    </tr>\n    <tr>\n      <th>2180</th>\n      <td>And he\\nsays there were four of them, Argus, P...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# Put tokenized sentences in dataframe\n",
    "\n",
    "Hesiod = DataFrame (sentences,columns=['tokenized_sentences'])\n",
    "Hesiod.sample(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column for lenth of sentences\n",
    "Hesiod[\"len\"] = Hesiod['tokenized_sentences'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulization of sentence lenght\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Hesiod[\"len\"], kde=False)"
   ]
  },
  {
   "source": [
    "# Pre-Processing \n",
    "There are many feature engineering strategies for transforming text data into features. Some involve assigning each unique word-like term to a feature and counting the number of occurrences per training example. However, if we were to perform this strategy right now, we'd end up with an absurd number of features, a result of the myriad possible terms. The classifier would take too long to train and likely overfit. As a result, each NLP problem requires a tailored approach to determine which terms are relevant and meaningful, and this is where we begin our pre-processing.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "can't get contractions installed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Tokenization\n",
    "\n",
    "Step 2: Tokenization\n",
    "\n",
    "In this step, we construct the features. We will begin by breaking apart the corpus into a vocabulary of unique terms, and this is called tokanization.\n",
    "\n",
    "We can tokenize individual terms and generate what's called a bag of words model. You may notice this model has a glaring pitfall: it fails to capture the innate structure of human language. We can also tokenize using nltk, which is the leading platform for building Python programs to work with human language data.\n",
    "\n",
    "We will begin my installing and importing nltk, so we can use it!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 tokenized_sentences  \\\n",
       "0  \\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...   \n",
       "1                         _The Works and Days_\\n II.   \n",
       "2  The Genealogical Poems\\n Date of the Hesiodic ...   \n",
       "3                                  TO DIONYSUS\\n II.   \n",
       "4                                  TO DEMETER\\n III.   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  [Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...  \n",
       "1                   [_The, Works, and, Days_, II, .]  \n",
       "2  [The, Genealogical, Poems, Date, of, the, Hesi...  \n",
       "3                              [TO, DIONYSUS, II, .]  \n",
       "4                              [TO, DEMETER, III, .]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokenized_sentences</th>\n      <th>word_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...</td>\n      <td>[Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_The Works and Days_\\n II.</td>\n      <td>[_The, Works, and, Days_, II, .]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Genealogical Poems\\n Date of the Hesiodic ...</td>\n      <td>[The, Genealogical, Poems, Date, of, the, Hesi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TO DIONYSUS\\n II.</td>\n      <td>[TO, DIONYSUS, II, .]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TO DEMETER\\n III.</td>\n      <td>[TO, DEMETER, III, .]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Hesiod['word_tokens'] = Hesiod['tokenized_sentences'].apply(word_tokenize)\n",
    "Hesiod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of values (62022) does not match length of index (3872)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1f4c26b2859f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mHesiod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens_no_stopwords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\LizPy\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3035\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3036\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3037\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3039\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LizPy\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3111\u001b[0m         \"\"\"\n\u001b[0;32m   3112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3113\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3114\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LizPy\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3757\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3758\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3759\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3760\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LizPy\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m    746\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    748\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (62022) does not match length of index (3872)"
     ]
    }
   ],
   "source": [
    "Hesiod['tokens_no_stopwords']= [word for word in tokens if not word in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 tokenized_sentences  \\\n",
       "0  \\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...   \n",
       "1                         _The Works and Days_\\n II.   \n",
       "2  The Genealogical Poems\\n Date of the Hesiodic ...   \n",
       "3                                  TO DIONYSUS\\n II.   \n",
       "4                                  TO DEMETER\\n III.   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...   \n",
       "1                   [_The, Works, and, Days_, II, .]   \n",
       "2  [The, Genealogical, Poems, Date, of, the, Hesi...   \n",
       "3                              [TO, DIONYSUS, II, .]   \n",
       "4                              [TO, DEMETER, III, .]   \n",
       "\n",
       "                                               lower  \n",
       "0  [hesiod, ,, the, homeric, hymns, ,, and, homer...  \n",
       "1                   [_the, works, and, days_, ii, .]  \n",
       "2  [the, genealogical, poems, date, of, the, hesi...  \n",
       "3                              [to, dionysus, ii, .]  \n",
       "4                              [to, demeter, iii, .]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokenized_sentences</th>\n      <th>word_tokens</th>\n      <th>lower</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...</td>\n      <td>[Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...</td>\n      <td>[hesiod, ,, the, homeric, hymns, ,, and, homer...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_The Works and Days_\\n II.</td>\n      <td>[_The, Works, and, Days_, II, .]</td>\n      <td>[_the, works, and, days_, ii, .]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Genealogical Poems\\n Date of the Hesiodic ...</td>\n      <td>[The, Genealogical, Poems, Date, of, the, Hesi...</td>\n      <td>[the, genealogical, poems, date, of, the, hesi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TO DIONYSUS\\n II.</td>\n      <td>[TO, DIONYSUS, II, .]</td>\n      <td>[to, dionysus, ii, .]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TO DEMETER\\n III.</td>\n      <td>[TO, DEMETER, III, .]</td>\n      <td>[to, demeter, iii, .]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# make lowercase\n",
    "Hesiod['lower'] = Hesiod['word_tokens'].apply(lambda x: [word.lower() for word in x])\n",
    "Hesiod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 tokenized_sentences  \\\n",
       "0  \\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...   \n",
       "1                         _The Works and Days_\\n II.   \n",
       "2  The Genealogical Poems\\n Date of the Hesiodic ...   \n",
       "3                                  TO DIONYSUS\\n II.   \n",
       "4                                  TO DEMETER\\n III.   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...   \n",
       "1                   [_The, Works, and, Days_, II, .]   \n",
       "2  [The, Genealogical, Poems, Date, of, the, Hesi...   \n",
       "3                              [TO, DIONYSUS, II, .]   \n",
       "4                              [TO, DEMETER, III, .]   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [hesiod, ,, the, homeric, hymns, ,, and, homer...   \n",
       "1                   [_the, works, and, days_, ii, .]   \n",
       "2  [the, genealogical, poems, date, of, the, hesi...   \n",
       "3                              [to, dionysus, ii, .]   \n",
       "4                              [to, demeter, iii, .]   \n",
       "\n",
       "                                             no_punc  \n",
       "0  [hesiod, the, homeric, hymns, and, homerica, b...  \n",
       "1                      [_the, works, and, days_, ii]  \n",
       "2  [the, genealogical, poems, date, of, the, hesi...  \n",
       "3                                 [to, dionysus, ii]  \n",
       "4                                 [to, demeter, iii]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokenized_sentences</th>\n      <th>word_tokens</th>\n      <th>lower</th>\n      <th>no_punc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...</td>\n      <td>[Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...</td>\n      <td>[hesiod, ,, the, homeric, hymns, ,, and, homer...</td>\n      <td>[hesiod, the, homeric, hymns, and, homerica, b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_The Works and Days_\\n II.</td>\n      <td>[_The, Works, and, Days_, II, .]</td>\n      <td>[_the, works, and, days_, ii, .]</td>\n      <td>[_the, works, and, days_, ii]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Genealogical Poems\\n Date of the Hesiodic ...</td>\n      <td>[The, Genealogical, Poems, Date, of, the, Hesi...</td>\n      <td>[the, genealogical, poems, date, of, the, hesi...</td>\n      <td>[the, genealogical, poems, date, of, the, hesi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TO DIONYSUS\\n II.</td>\n      <td>[TO, DIONYSUS, II, .]</td>\n      <td>[to, dionysus, ii, .]</td>\n      <td>[to, dionysus, ii]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TO DEMETER\\n III.</td>\n      <td>[TO, DEMETER, III, .]</td>\n      <td>[to, demeter, iii, .]</td>\n      <td>[to, demeter, iii]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# take out punctionation\n",
    "import string\n",
    "punc = string.punctuation\n",
    "Hesiod['no_punc'] = Hesiod['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "Hesiod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\lizba\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 tokenized_sentences  \\\n",
       "0  \\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...   \n",
       "1                         _The Works and Days_\\n II.   \n",
       "2  The Genealogical Poems\\n Date of the Hesiodic ...   \n",
       "3                                  TO DIONYSUS\\n II.   \n",
       "4                                  TO DEMETER\\n III.   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...   \n",
       "1                   [_The, Works, and, Days_, II, .]   \n",
       "2  [The, Genealogical, Poems, Date, of, the, Hesi...   \n",
       "3                              [TO, DIONYSUS, II, .]   \n",
       "4                              [TO, DEMETER, III, .]   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [hesiod, ,, the, homeric, hymns, ,, and, homer...   \n",
       "1                   [_the, works, and, days_, ii, .]   \n",
       "2  [the, genealogical, poems, date, of, the, hesi...   \n",
       "3                              [to, dionysus, ii, .]   \n",
       "4                              [to, demeter, iii, .]   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [hesiod, the, homeric, hymns, and, homerica, b...   \n",
       "1                      [_the, works, and, days_, ii]   \n",
       "2  [the, genealogical, poems, date, of, the, hesi...   \n",
       "3                                 [to, dionysus, ii]   \n",
       "4                                 [to, demeter, iii]   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [hesiod, homeric, hymns, homerica, homer, hesi...  \n",
       "1                           [_the, works, days_, ii]  \n",
       "2  [genealogical, poems, date, hesiodic, poems, l...  \n",
       "3                                     [dionysus, ii]  \n",
       "4                                     [demeter, iii]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokenized_sentences</th>\n      <th>word_tokens</th>\n      <th>lower</th>\n      <th>no_punc</th>\n      <th>stopwords_removed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n\\n\\n\\n\\nHesiod, The Homeric Hymns, and Homer...</td>\n      <td>[Hesiod, ,, The, Homeric, Hymns, ,, and, Homer...</td>\n      <td>[hesiod, ,, the, homeric, hymns, ,, and, homer...</td>\n      <td>[hesiod, the, homeric, hymns, and, homerica, b...</td>\n      <td>[hesiod, homeric, hymns, homerica, homer, hesi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_The Works and Days_\\n II.</td>\n      <td>[_The, Works, and, Days_, II, .]</td>\n      <td>[_the, works, and, days_, ii, .]</td>\n      <td>[_the, works, and, days_, ii]</td>\n      <td>[_the, works, days_, ii]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Genealogical Poems\\n Date of the Hesiodic ...</td>\n      <td>[The, Genealogical, Poems, Date, of, the, Hesi...</td>\n      <td>[the, genealogical, poems, date, of, the, hesi...</td>\n      <td>[the, genealogical, poems, date, of, the, hesi...</td>\n      <td>[genealogical, poems, date, hesiodic, poems, l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TO DIONYSUS\\n II.</td>\n      <td>[TO, DIONYSUS, II, .]</td>\n      <td>[to, dionysus, ii, .]</td>\n      <td>[to, dionysus, ii]</td>\n      <td>[dionysus, ii]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TO DEMETER\\n III.</td>\n      <td>[TO, DEMETER, III, .]</td>\n      <td>[to, demeter, iii, .]</td>\n      <td>[to, demeter, iii]</td>\n      <td>[demeter, iii]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# take outstop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "Hesiod['stopwords_removed'] = Hesiod['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "Hesiod.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/348/348-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request.urlopen(url)\n",
    "text = response\n",
    "text.words\n",
    "#url = 'http://www.gutenberg.org/files/348/348-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}